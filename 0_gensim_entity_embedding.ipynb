{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntodo\\n三种方法：\\n1 gensim 训练\\u3000embedding\\n2 openKE 训练\\u3000embedding\\n3 openKE\\u3000分词训练\\u3000embedding\\u3000\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "todo\n",
    "三种方法：\n",
    "1 gensim 训练　embedding\n",
    "2 openKE 训练　embedding\n",
    "3 openKE　分词训练　embedding　\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:u8\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as tqdm\n",
    "from gensim.models import word2vec\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取知识库\n",
    "kb_data = []\n",
    "kb = {}\n",
    "with open('data/raw_data/kb_data','r') as f:\n",
    "    for line in f:\n",
    "        item = eval(str(json.loads(line)).lower())\n",
    "        kb[item['subject_id']] = item\n",
    "        kb_data.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenKE 默认方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate = {}\n",
    "subject = {}\n",
    "type_object = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in tqdm(kb_data):\n",
    "#     if item['subject_id'] not in subject:\n",
    "#         subject[item['subject_id']] = len(subject)\n",
    "#     for t in item['data']:\n",
    "#         if t['predicate'] == '摘要':continue\n",
    "#         if t['predicate'] not in predicate:\n",
    "#             predicate[t['predicate']] = len(predicate)\n",
    "#         if t['object'] not in type_object:\n",
    "#             type_object[t['object']] = len(type_object)\n",
    "#     for t_type in item['type']:\n",
    "#         if t_type not in type_object:\n",
    "#             type_object[t_type] = len(type_object)\n",
    "# if 'type' not in predicate:\n",
    "#     predicate['type'] = len(predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spo = []\n",
    "# for item in tqdm(kb_data):\n",
    "#     subject_id = subject[item['subject_id']]\n",
    "#     for t_type in item['type']:\n",
    "#         spo.append([subject_id,predicate['type'],type_object[t_type]])\n",
    "#     for p in item['data']:\n",
    "#         if p['predicate'] == '摘要':continue\n",
    "#         spo.append([subject_id,predicate[p['predicate']],type_object[p['object']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('entity_embedding/train2id.txt','w') as f:\n",
    "#     #0 1 0\n",
    "#     f.write(str(len(spo))+'\\n')\n",
    "#     for spo_line in spo:\n",
    "#         f.write(str(spo_line[0])+' '+str(spo_line[1])+' '+str(spo_line[2])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 14951\n",
    "# # /m/027rn\t0\n",
    "# with open('entity_embedding/entity2id.txt','w') as f:\n",
    "#     f.write(str(len(type_object))+'\\n')\n",
    "#     for key in type_object:\n",
    "#         f.write(str(key)+' '+str(type_object[key])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('entity_embedding/relation2id.txt','w') as f:\n",
    "#     f.write(str(len(predicate))+'\\n')\n",
    "#     for key in predicate:\n",
    "#         f.write(str(key)+' '+str(predicate[key])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/399252 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.588 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "100%|██████████| 399252/399252 [00:03<00:00, 114896.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# jieba加字典\n",
    "for item in tqdm(kb_data):\n",
    "    jieba.add_word(item['subject'],freq=10000)\n",
    "    for name in item['alias']:\n",
    "        jieba.add_word(name, freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399252/399252 [03:57<00:00, 1683.55it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentence = []\n",
    "for item in tqdm(kb_data):\n",
    "    sentence.append([item['subject_id'],item['type'][0]])\n",
    "    for t in item['data']:\n",
    "        temp = []\n",
    "        if t['predicate'] == '摘要':\n",
    "            abstract = jieba.lcut(t['object'])\n",
    "            for word in abstract:\n",
    "                if word !=item['subject']:\n",
    "                    temp.append(word)\n",
    "                else:\n",
    "                    temp.append(item['subject_id'])\n",
    "            sentence.append(temp)\n",
    "        else:\n",
    "            temp.append(item['subject_id'])\n",
    "            temp.append(t['predicate'])\n",
    "            for w in jieba.lcut(t['object']):\n",
    "                temp.append(w)\n",
    "            sentence.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open('data/raw_data/train.json','r') as f:\n",
    "    for line in f:\n",
    "        sentence.append(jieba.lcut(eval(str(json.loads(line)).lower())['text']))\n",
    "with open('data/raw_data/develop.json','r') as f:\n",
    "    for line in f:\n",
    "        sentence.append(jieba.lcut(eval(str(json.loads(line)).lower())['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.06740022e+00 -1.30582309e+00 -6.24896049e-01  2.74170566e+00\n",
      " -1.32954881e-01 -7.22744644e-01  9.90687966e-01  2.22109914e+00\n",
      " -6.25711977e-01  6.69428349e-01  2.50128895e-01  2.34205270e+00\n",
      " -9.85711932e-01  1.75330555e+00  1.75123736e-02 -6.24078929e-01\n",
      "  5.50190394e-04 -1.19829345e+00  1.71348405e+00  6.60451233e-01\n",
      " -7.05054700e-01 -2.52027655e+00 -2.49819350e+00 -9.74730909e-01\n",
      "  1.05229843e+00 -1.58508432e+00  7.35398948e-01  3.09939623e+00\n",
      " -1.80983472e+00 -2.25609541e+00  1.69330287e+00  1.97928935e-01\n",
      " -2.87188470e-01 -1.55438244e+00 -2.25666419e-01 -1.64868927e+00\n",
      " -2.28676736e-01  1.82918414e-01 -2.08738875e-02  1.31223845e+00\n",
      "  9.26504910e-01 -4.16131198e-01 -4.47655112e-01 -6.56702995e-01\n",
      " -3.93060774e-01 -2.33329311e-01 -9.28635970e-02 -7.27430761e-01\n",
      "  5.80050230e-01  6.67945683e-01  6.72419012e-01 -1.15615800e-01\n",
      " -2.58462161e-01  1.58006990e+00  8.96789134e-01 -1.67929089e+00\n",
      " -2.91498989e-01 -1.08625698e+00  1.92022491e+00  1.66703090e-01\n",
      " -1.00618199e-01 -2.27684036e-01  5.85815981e-02  3.06457162e-01\n",
      " -1.23699570e+00  8.76747072e-01  1.01262331e+00  2.49714231e+00\n",
      "  3.46369371e-02  8.89135540e-01  1.91726494e+00  4.86710340e-01\n",
      "  3.26317936e-01 -8.27590823e-01  1.09412804e-01  1.85802054e+00\n",
      " -6.06806934e-01 -1.17489421e+00 -9.15060401e-01  9.89948034e-01\n",
      "  2.95094281e-01  3.35611403e-02 -5.19397974e-01  1.41882241e-01\n",
      "  7.14128137e-01  1.69615149e+00  9.67213035e-01 -3.47485214e-01\n",
      " -7.15998590e-01  5.61189473e-01 -1.17827702e+00 -1.38819444e+00\n",
      " -2.07913160e-01 -4.24022585e-01  8.21767390e-01 -9.06645179e-01\n",
      " -1.05670142e+00 -7.34023690e-01  2.20503974e+00  2.23521185e+00]\n",
      "[('beautiful', 0.9206537008285522), ('heart', 0.9007788896560669)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhukaihua/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentence, size=100, window=5, min_count=1, workers=10)\n",
    "# 打印单词'good'的词向量\n",
    "print(model.wv.word_vec('good'))\n",
    "# 打印和'good'相似的前2个单词\n",
    "print(model.wv.most_similar('good', topn=2))\n",
    "# 保存模型到文件\n",
    "model.save('embedding/w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhukaihua/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83644915 -0.13179712 -0.32689977  1.4807096   0.26997417 -0.7888522\n",
      "  0.18786314 -1.0655216   0.15017082  0.7930093   0.6204069  -0.12830432\n",
      "  0.24433438  0.85823154  0.02746648  0.5008542  -0.23827039  0.58007616\n",
      "  1.7565713  -1.0904726   1.9679656  -1.5737779   1.3745987   0.76041174\n",
      "  0.19700734  2.0845032   0.71697253 -0.81742346 -0.75465655  0.43983054\n",
      " -0.91081196 -0.12020243  2.330965   -0.9543566  -1.795786   -1.2433215\n",
      "  0.06511574 -1.0525746   1.1140516  -0.2883513   1.0940838   0.75270617\n",
      "  0.4522416  -0.5509725  -1.1325698  -1.7036166  -1.9608523  -0.38575265\n",
      "  0.5976845   1.623124   -0.4178196   0.6338049  -0.9562957   1.2905524\n",
      "  0.28920326 -1.1495526   0.08914449  0.2507475  -1.615023    0.34573108\n",
      " -0.08599114  1.1669557   0.39803565 -0.66602045 -0.05413941  0.61265546\n",
      "  1.4831152   2.579434   -0.7199152  -0.49818987 -0.39623013 -0.8251795\n",
      "  1.5805804  -2.870432   -0.7189991  -1.184574   -0.6879349  -1.5456197\n",
      " -0.9740356  -0.7846144  -0.40417776  0.4827905   0.9376345   0.7471991\n",
      " -0.02944718  0.77415437  0.13420272  0.54730046 -0.7837808   0.15465294\n",
      "  0.19209586  0.13738085 -0.9137943  -2.6965737  -1.4851967   0.44742823\n",
      "  0.06711761  1.4254432   1.6096506   1.6285206 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "entity_embedding = Word2Vec.load('embedding/w2v.model')\n",
    "print(entity_embedding.wv.word_vec('搜索'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
