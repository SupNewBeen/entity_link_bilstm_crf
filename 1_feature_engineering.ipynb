{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:u8\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "import gc,jieba\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = []\n",
    "# train_data_dict = {}\n",
    "# with open('data/raw_data/train.json','r') as f:\n",
    "#     for line in f:\n",
    "#         raw = json.loads(line)\n",
    "#         train_data.append(raw)\n",
    "#         train_data_dict[raw['text_id']] = raw\n",
    "# # 读取知识库\n",
    "# kb_data = []\n",
    "# kb = {}\n",
    "# with open('data/raw_data/kb_data','r') as f:\n",
    "#     for line in f:\n",
    "#         item = json.loads(line)\n",
    "#         kb[item['subject_id']] = item\n",
    "#         kb_data.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_data_dict = {}\n",
    "with open('data/raw_data/train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        raw = eval(str(json.loads(line)).lower())\n",
    "        train_data.append(raw)\n",
    "        #train_data_dict[raw['text_id']] = raw\n",
    "kb = {}\n",
    "with open('data/raw_data/kb_data', 'r') as f:\n",
    "    for line in f:\n",
    "        item = eval(str(json.loads(line)).lower())\n",
    "        kb[item['subject_id']] = item\n",
    "\n",
    "name_id = {}\n",
    "for kb_id in kb:\n",
    "    for item in kb[kb_id]['alias']:\n",
    "        if item not in name_id:\n",
    "            name_id[item] = [kb_id]\n",
    "        else:\n",
    "            name_id[item].append(kb_id)\n",
    "    if kb[kb_id]['subject'] not in name_id:\n",
    "        name_id[kb[kb_id]['subject']] = [kb_id]\n",
    "    else:\n",
    "        name_id[kb[kb_id]['subject']].append(kb_id)\n",
    "for id in name_id:\n",
    "    name_id[id] = sorted(list(set(name_id[id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO:\\n预测type,\\n引入tfidf\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "预测type,\n",
    "引入tfidf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_word(a, b):\n",
    "        common_count = 0\n",
    "        for word in a:\n",
    "            if word in b:\n",
    "                common_count += 1\n",
    "        return common_count\n",
    "def cal_similarity(params):\n",
    "    index,sentence_a,sentence_b = params\n",
    "    la = len(sentence_a)\n",
    "    lb = len(sentence_b)\n",
    "\n",
    "    char_a = [c for c in sentence_a]\n",
    "    char_b = [c for c in sentence_b]\n",
    "    word_a = jieba.lcut_for_search(sentence_a)\n",
    "    word_b = jieba.lcut_for_search(sentence_b)\n",
    "    bigram_a = [sentence_a[i:i + 2] for i in range(len(sentence_a) - 1)]\n",
    "    bigram_b = [sentence_b[i:i + 2] for i in range(len(sentence_b) - 1)]\n",
    "    trigram_a = [sentence_a[i:i + 3] for i in range(len(sentence_a) - 2)]\n",
    "    trigram_b = [sentence_b[i:i + 3] for i in range(len(sentence_b) - 2)]\n",
    "\n",
    "    common_word, common_word_score = 0, 0\n",
    "    for w in word_b:\n",
    "        if w in word_a:\n",
    "            common_word += 1\n",
    "            common_word_score += 1\n",
    "\n",
    "    common_c = get_common_word(char_a, char_b)\n",
    "    common_bigram = get_common_word(bigram_a, bigram_b)\n",
    "    common_trigram = get_common_word(trigram_a, trigram_b)\n",
    "\n",
    "    max_len = max(la, lb)\n",
    "    min_len = min(la, lb)\n",
    "    if max_len==0:\n",
    "        max_len=0.01\n",
    "    if min_len==0:\n",
    "        min_len = 0.01\n",
    "    numerical_feats = [max_len, min_len, common_word, common_word_score,\n",
    "                                common_word / min_len, common_word / max_len,\n",
    "                                common_word_score / min_len, common_word_score / max_len,\n",
    "                                common_c / min_len, common_c / max_len,\n",
    "                                common_bigram / min_len, common_bigram / max_len,\n",
    "                                common_trigram / min_len, common_trigram / max_len,\n",
    "                                common_word_score - common_word]\n",
    "    return index, numerical_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 书名号纠错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:20<00:00, 4431.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取candidate候选特征\n",
    "kb_column = []\n",
    "train_column = []\n",
    "text_id = []\n",
    "label_colum = []\n",
    "num_attrs = []\n",
    "num_abstract_words = []\n",
    "num_alias = []\n",
    "m_id = 0\n",
    "m_id_list = []\n",
    "num_candidate = []\n",
    "equal_subject = []\n",
    "shuminghao = [] \n",
    "entity_common = []\n",
    "len_mention = []\n",
    "mention_start = []\n",
    "mention_end = []\n",
    "numerical_f = []\n",
    "is_eng = []\n",
    "create_works = []\n",
    "end_with_point = []\n",
    "end_with_line = []\n",
    "end_with_line2 = []\n",
    "end_with_shuminghao = []\n",
    "for s in tqdm(train_data):\n",
    "    mention_ner = s['mention_data']\n",
    "    m_list = []\n",
    "    for m in mention_ner:\n",
    "        m_list.append(m['mention'])\n",
    "    for m in mention_ner:\n",
    "        \n",
    "        if m['mention'] not in name_id : # 不能注销，因为有些实体没有or m['kb_id']=='nil':\n",
    "            continue\n",
    "        if m['kb_id'] != 'nil':\n",
    "            name_list = [kb[m['kb_id']]['subject']]+kb[m['kb_id']]['alias']\n",
    "            if m['mention'] not in name_list:\n",
    "                    continue\n",
    "\n",
    "        candidate_ids = name_id[m['mention']]\n",
    "        for m_candidate_id in candidate_ids:\n",
    "            # 统计配对特征\n",
    "            candidate_numattrs = 0\n",
    "            candidate_abstract_numwords = 0\n",
    "            candidate_detail = kb[m_candidate_id]\n",
    "            candi_text = ''\n",
    "            for predicate in candidate_detail['data']:\n",
    "                candidate_numattrs += 1\n",
    "                candi_text += predicate['object']\n",
    "                #candi_text += predicate['predicate']\n",
    "\n",
    "                if predicate['predicate'] == '摘要':\n",
    "                    candidate_abstract = predicate['object']\n",
    "                    candidate_abstract_numwords = len(candidate_abstract)\n",
    "#                 if predicate['predicate'] == '标签':\n",
    "#                     candidate_label += predicate['object']\n",
    "            if m_candidate_id==m['kb_id']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            train_column.append(m['mention'])\n",
    "            kb_column.append(m_candidate_id)\n",
    "            text_id.append(s['text_id'])\n",
    "            label_colum.append(label)\n",
    "            num_attrs.append(candidate_numattrs)\n",
    "            num_abstract_words.append(candidate_abstract_numwords)\n",
    "            candidate_name_list = candidate_detail['alias']+[candidate_detail['subject']]\n",
    "            num_alias.append(len(set(candidate_name_list)))\n",
    "            m_id_list.append(m_id)\n",
    "            num_candidate.append(len(candidate_ids))\n",
    "            equal_subject.append(candidate_detail['subject']==m['mention'])\n",
    "            \n",
    "            shu_in_abstract = False\n",
    "            for sub_alia in candidate_name_list:\n",
    "                if '《'+sub_alia+'》' in candidate_abstract:\n",
    "                    shu_in_abstract = True\n",
    "            shuminghao.append(shu_in_abstract)\n",
    "            \n",
    "#             if langid.classify(m['mention'])[0]=='en':\n",
    "#                 len_mention.append(len(m['mention'].split()))\n",
    "#                 is_eng.append(1)\n",
    "#             else:\n",
    "#                 len_mention.append(len(m['mention']))\n",
    "#                 is_eng.append(0)\n",
    "            len_mention.append(len(m['mention']))\n",
    "            mention_start.append(m['offset'])\n",
    "            mention_end.append(len(s['text'])-int(m['offset'])-len(m['mention']))\n",
    "            \n",
    "            \n",
    "            i = 0\n",
    "            for item in m_list:\n",
    "                if item in candi_text: i+=1\n",
    "            entity_common.append(i)\n",
    "            create_works.append('《'+m['mention']+'》' in s['text'] )\n",
    "            end_with_point.append('.' == s['text'][-1])\n",
    "            end_with_line.append('-' in s['text'])\n",
    "            end_with_line2.append('_' in s['text'])\n",
    "            end_with_shuminghao.append('《' in s['text'])\n",
    "        m_id += 1\n",
    "        \n",
    "data = pd.DataFrame()\n",
    "data['text_id'] = text_id\n",
    "data['kb_id'] = kb_column\n",
    "data['train_mention'] = train_column\n",
    "data['label'] = label_colum\n",
    "data['num_attrs'] = num_attrs\n",
    "data['num_abstract_words'] = num_abstract_words\n",
    "data['num_alias'] = num_alias\n",
    "data['m_id'] = m_id_list\n",
    "data['num_candidates'] = num_candidate \n",
    "data['mention_equal_subject'] = equal_subject\n",
    "data['shuminghao'] = shuminghao\n",
    "data['entity_common']  = entity_common\n",
    "data['len_mention'] = len_mention\n",
    "data['mention_start'] = list(map(int,mention_start))\n",
    "data['create_works'] = create_works\n",
    "data['mention_end'] = mention_end\n",
    "data['end_with_point'] = end_with_point\n",
    "data['end_with_line'] = end_with_line\n",
    "data['end_with_line2'] = end_with_line2\n",
    "data['end_with_shuminghao'] = end_with_shuminghao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1566680, 20)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8854.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取candidate候选特征\n",
    "numerical_f = []\n",
    "i = 0\n",
    "for s in tqdm(train_data):\n",
    "    mention_ner = s['mention_data']\n",
    "    m_list = []\n",
    "    for m in mention_ner:\n",
    "        m_list.append(m['mention'])\n",
    "    for m in mention_ner:\n",
    "        if m['mention'] not in name_id : # 不能注销，因为有些实体没有or m['kb_id']=='nil':\n",
    "            continue\n",
    "        if m['kb_id'] != 'nil':\n",
    "            name_list = [kb[m['kb_id']]['subject']]+kb[m['kb_id']]['alias']\n",
    "            if m['mention'] not in name_list:\n",
    "                    continue\n",
    "        \n",
    "        candidate_ids = name_id[m['mention']]\n",
    "#         add_extra = 0 \n",
    "#         if m['kb_id'] != 'nil':\n",
    "#             name_list = [kb[m['kb_id']]['subject']]+kb[m['kb_id']]['alias']\n",
    "            \n",
    "#             if m['mention'] not in name_list:\n",
    "#                 keep = False\n",
    "#                 for name_c in ''.join(name_list):\n",
    "#                     if name_c in m['mention']:\n",
    "#                         #print(name_c,m['mention'])\n",
    "#                         keep = True\n",
    "#                         break\n",
    "#                 if not keep:\n",
    "#                     continue\n",
    "#                 candidate_ids = name_id[m['mention']]+[m['kb_id']]\n",
    "#                 add_extra = 1\n",
    "        for m_candidate_id in candidate_ids:\n",
    "            # 统计配对特征\n",
    "            candidate_numattrs = 0\n",
    "            candidate_abstract_numwords = 0\n",
    "            candidate_detail = kb[m_candidate_id]\n",
    "            candi_text = ''\n",
    "            for predicate in candidate_detail['data']:\n",
    "                candidate_numattrs += 1\n",
    "                candi_text += predicate['object']\n",
    "                #candi_text += predicate['predicate']\n",
    "            numerical_f.append((i,candi_text,s['text']))\n",
    "            i+=1\n",
    "            #numerical_f.append(cal_similarity(candi_text,s['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.860 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.913 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.950 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.671 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.873 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.276 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "p = multiprocessing.Pool(6)\n",
    "res = p.map(cal_similarity, numerical_f)\n",
    "res = sorted(res,key=lambda x:x[0],reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nume_f = [x[1] for x in res]\n",
    "numerical_f = np.array(nume_f)\n",
    "for i in range(numerical_f.shape[1]):\n",
    "    data['numerical_%d'%i] = numerical_f[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>kb_id</th>\n",
       "      <th>train_mention</th>\n",
       "      <th>label</th>\n",
       "      <th>num_attrs</th>\n",
       "      <th>num_abstract_words</th>\n",
       "      <th>num_alias</th>\n",
       "      <th>m_id</th>\n",
       "      <th>num_candidates</th>\n",
       "      <th>mention_equal_subject</th>\n",
       "      <th>...</th>\n",
       "      <th>numerical_5</th>\n",
       "      <th>numerical_6</th>\n",
       "      <th>numerical_7</th>\n",
       "      <th>numerical_8</th>\n",
       "      <th>numerical_9</th>\n",
       "      <th>numerical_10</th>\n",
       "      <th>numerical_11</th>\n",
       "      <th>numerical_12</th>\n",
       "      <th>numerical_13</th>\n",
       "      <th>numerical_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>130287</td>\n",
       "      <td>南京南站</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>338</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018648</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.018648</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>0.186480</td>\n",
       "      <td>1.736842</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.037296</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>311223</td>\n",
       "      <td>南京南站</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>447</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012638</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.012638</td>\n",
       "      <td>6.473684</td>\n",
       "      <td>0.194313</td>\n",
       "      <td>2.473684</td>\n",
       "      <td>0.074250</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>0.031596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>341096</td>\n",
       "      <td>高铁</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>1.473684</td>\n",
       "      <td>0.088050</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>130287</td>\n",
       "      <td>南京南站</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>338</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018648</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.018648</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>0.186480</td>\n",
       "      <td>1.736842</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.037296</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>311223</td>\n",
       "      <td>南京南站</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>447</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012638</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.012638</td>\n",
       "      <td>6.473684</td>\n",
       "      <td>0.194313</td>\n",
       "      <td>2.473684</td>\n",
       "      <td>0.074250</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>0.031596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  text_id   kb_id train_mention  label  num_attrs  num_abstract_words  \\\n",
       "0       1  130287          南京南站      0          9                 338   \n",
       "1       1  311223          南京南站      1         17                 447   \n",
       "2       1  341096            高铁      1         10                 239   \n",
       "3       1  130287          南京南站      0          9                 338   \n",
       "4       1  311223          南京南站      1         17                 447   \n",
       "\n",
       "   num_alias  m_id  num_candidates  mention_equal_subject  ...  numerical_5  \\\n",
       "0          2     0               2                   True  ...     0.018648   \n",
       "1          3     0               2                   True  ...     0.012638   \n",
       "2          1     1               1                   True  ...     0.009434   \n",
       "3          2     2               2                   True  ...     0.018648   \n",
       "4          3     2               2                   True  ...     0.012638   \n",
       "\n",
       "   numerical_6  numerical_7  numerical_8  numerical_9  numerical_10  \\\n",
       "0     0.421053     0.018648     4.210526     0.186480      1.736842   \n",
       "1     0.421053     0.012638     6.473684     0.194313      2.473684   \n",
       "2     0.157895     0.009434     1.473684     0.088050      0.315789   \n",
       "3     0.421053     0.018648     4.210526     0.186480      1.736842   \n",
       "4     0.421053     0.012638     6.473684     0.194313      2.473684   \n",
       "\n",
       "   numerical_11  numerical_12  numerical_13  numerical_14  \n",
       "0      0.076923      0.842105      0.037296           0.0  \n",
       "1      0.074250      1.052632      0.031596           0.0  \n",
       "2      0.018868      0.000000      0.000000           0.0  \n",
       "3      0.076923      0.842105      0.037296           0.0  \n",
       "4      0.074250      1.052632      0.031596           0.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_columns = ['numerical_%d'%i for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[f_columns].to_pickle('data/numerical_f.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate　type的one-hot特征\n",
    "data['type'] = data.apply(lambda x:kb[x['kb_id']]['type'][0],axis=1)\n",
    "#data = pd.concat([data,pd.get_dummies(data['type'])],axis=1)\n",
    "\n",
    "# target encoding\n",
    "groupbytype = data.groupby('type',as_index=False)['label'].agg({\n",
    "    'type_label_mean':'mean','type_label_count':'count'})\n",
    "data = data.merge(groupbytype,on='type',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/step1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mid = data['m_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 戴安楠ner\n",
    "ner_result = pd.read_csv('result/el_dev_result(1).csv',sep='\\t')\n",
    "ner_result['offset'] = ner_result['offset'].apply(lambda x:eval(x.lower()))\n",
    "ner_result['mention'] = ner_result['mention'].apply(lambda x:eval(x.lower()))\n",
    "ner_result = ner_result.rename(columns={'mention':'pred','offset':'pos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>《中国新歌声》刘天文曾参加“快乐男声” 刘天文 ...</td>\n",
       "      <td>[中国新歌声, 刘天文, 快乐男声, 刘天文]</td>\n",
       "      <td>[1, 7, 14, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>《林学概论》_简介</td>\n",
       "      <td>[林学概论, 简介]</td>\n",
       "      <td>[1, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>小彩旗13岁 与杨丽萍表演舞蹈《春》</td>\n",
       "      <td>[小彩旗, 杨丽萍, 舞蹈, 春]</td>\n",
       "      <td>[0, 8, 13, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>《了不起的盖茨比》「原著改编」繁华世界虚伪的梦</td>\n",
       "      <td>[了不起的盖茨比, 原著, 世界]</td>\n",
       "      <td>[1, 10, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>《古剑奇谭二》正式启动:拒绝山寨特效 剧情不再狗血</td>\n",
       "      <td>[古剑奇谭二, 山寨, 特效, 剧情]</td>\n",
       "      <td>[1, 14, 16, 19]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                         text                     pred  \\\n",
       "0        1  《中国新歌声》刘天文曾参加“快乐男声” 刘天文 ...  [中国新歌声, 刘天文, 快乐男声, 刘天文]   \n",
       "1        2                    《林学概论》_简介               [林学概论, 简介]   \n",
       "2        3           小彩旗13岁 与杨丽萍表演舞蹈《春》        [小彩旗, 杨丽萍, 舞蹈, 春]   \n",
       "3        4      《了不起的盖茨比》「原著改编」繁华世界虚伪的梦        [了不起的盖茨比, 原著, 世界]   \n",
       "4        5    《古剑奇谭二》正式启动:拒绝山寨特效 剧情不再狗血      [古剑奇谭二, 山寨, 特效, 剧情]   \n",
       "\n",
       "               pos  \n",
       "0   [1, 7, 14, 20]  \n",
       "1           [1, 7]  \n",
       "2   [0, 8, 13, 16]  \n",
       "3      [1, 10, 17]  \n",
       "4  [1, 14, 16, 19]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包装成和train_data一样的格式\n",
    "test_data = []\n",
    "for index,row in ner_result.iterrows():\n",
    "    item = {}\n",
    "    item['text_id'] = row['text_id']\n",
    "    item['text'] = row['text']\n",
    "    item['mention_data'] = []\n",
    "    for i,p in enumerate(row['pos']):\n",
    "        item['mention_data'].append({'mention':row['pred'][i], 'offset':str(p)})\n",
    "    test_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:06<00:00, 3721.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取candidate候选特征\n",
    "kb_column = []\n",
    "train_column = []\n",
    "text_id = []\n",
    "num_attrs = []\n",
    "num_abstract_words = []\n",
    "num_alias = []\n",
    "m_id = 0\n",
    "m_id_list = []\n",
    "num_candidate = []\n",
    "equal_subject = []\n",
    "shuminghao = [] \n",
    "entity_common = []\n",
    "len_mention = []\n",
    "mention_start = []\n",
    "mention_end = []\n",
    "numerical_f = []\n",
    "is_eng = []\n",
    "create_works = []\n",
    "end_with_point = []\n",
    "end_with_line = []\n",
    "end_with_line2 = []\n",
    "end_with_shuminghao = []\n",
    "for s in tqdm(test_data):\n",
    "    mention_ner = s['mention_data']\n",
    "    m_list = []\n",
    "    for m in mention_ner:\n",
    "        \n",
    "        if m['mention'] not in name_id : # 不能注销，因为有些实体没有or m['kb_id']=='nil':\n",
    "            continue\n",
    "        candidate_ids = name_id[m['mention']]\n",
    "        for m_candidate_id in candidate_ids:\n",
    "            # 统计配对特征\n",
    "            candidate_numattrs = 0\n",
    "            candidate_abstract_numwords = 0\n",
    "            candidate_detail = kb[m_candidate_id]\n",
    "            candi_text = ''\n",
    "            for predicate in candidate_detail['data']:\n",
    "                candidate_numattrs += 1\n",
    "                candi_text += predicate['object']\n",
    "                #candi_text += predicate['predicate']\n",
    "                if predicate['predicate'] == '摘要':\n",
    "                    candidate_abstract = predicate['object']\n",
    "                    candidate_abstract_numwords = len(candidate_abstract)\n",
    "#                 if predicate['predicate'] == '标签':\n",
    "#                     candidate_label += predicate['object']\n",
    "            train_column.append(m['mention'])\n",
    "            kb_column.append(m_candidate_id)\n",
    "            num_attrs.append(candidate_numattrs)\n",
    "            num_abstract_words.append(candidate_abstract_numwords)\n",
    "            candidate_name_list = candidate_detail['alias']+[candidate_detail['subject']]\n",
    "            num_alias.append(len(set(candidate_name_list)))\n",
    "            m_id_list.append(m_id)\n",
    "            num_candidate.append(len(candidate_ids))\n",
    "            equal_subject.append(candidate_detail['subject']==m['mention'])\n",
    "            \n",
    "            shu_in_abstract = False\n",
    "            for sub_alia in candidate_name_list:\n",
    "                if '《'+sub_alia+'》' in candidate_abstract:\n",
    "                    shu_in_abstract = True\n",
    "            shuminghao.append(shu_in_abstract)\n",
    "            \n",
    "#             if langid.classify(m['mention'])[0]=='en':\n",
    "#                 len_mention.append(len(m['mention'].split()))\n",
    "#                 is_eng.append(1)\n",
    "#             else:\n",
    "#                 len_mention.append(len(m['mention']))\n",
    "#                 is_eng.append(0)\n",
    "            len_mention.append(len(m['mention']))\n",
    "            mention_start.append(m['offset'])\n",
    "            mention_end.append(len(s['text'])-int(m['offset'])-len(m['mention']))\n",
    "            \n",
    "            \n",
    "            i = 0\n",
    "            for item in m_list:\n",
    "                if item in candi_text: i+=1\n",
    "            entity_common.append(i)\n",
    "            create_works.append('《'+m['mention']+'》' in s['text'] )\n",
    "            end_with_point.append('.' == s['text'][-1])\n",
    "            end_with_line.append('-' in s['text'])\n",
    "            end_with_line2.append('_' in s['text'])\n",
    "            end_with_shuminghao.append('《' in s['text'])\n",
    "        m_id += 1\n",
    "        \n",
    "data = pd.DataFrame()\n",
    "data['text_id'] = text_id\n",
    "data['kb_id'] = kb_column\n",
    "data['train_mention'] = train_column\n",
    "data['num_attrs'] = num_attrs\n",
    "data['num_abstract_words'] = num_abstract_words\n",
    "data['num_alias'] = num_alias\n",
    "data['m_id'] = m_id_list\n",
    "data['num_candidates'] = num_candidate \n",
    "data['mention_equal_subject'] = equal_subject\n",
    "data['shuminghao'] = shuminghao\n",
    "data['entity_common']  = entity_common\n",
    "data['len_mention'] = len_mention\n",
    "data['mention_start'] = list(map(int,mention_start))\n",
    "data['create_works'] = create_works\n",
    "data['mention_end'] = mention_end\n",
    "data['end_with_point'] = end_with_point\n",
    "data['end_with_line'] = end_with_line\n",
    "data['end_with_line2'] = end_with_line2\n",
    "data['end_with_shuminghao'] = end_with_shuminghao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:03<00:00, 7731.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取candidate候选特征\n",
    "numerical_f = []\n",
    "i = 0\n",
    "for s in tqdm(test_data):\n",
    "    mention_ner = s['mention_data']\n",
    "    m_list = []\n",
    "    for m in mention_ner:\n",
    "        m_list.append(m['mention'])\n",
    "    for m in mention_ner:\n",
    "        if m['mention'] not in name_id : # 不能注销，因为有些实体没有or m['kb_id']=='nil':\n",
    "            continue\n",
    "        candidate_ids = name_id[m['mention']]\n",
    "\n",
    "        for m_candidate_id in candidate_ids:\n",
    "            # 统计配对特征\n",
    "            candidate_numattrs = 0\n",
    "            candidate_abstract_numwords = 0\n",
    "            candidate_detail = kb[m_candidate_id]\n",
    "            candi_text = ''\n",
    "            for predicate in candidate_detail['data']:\n",
    "                candidate_numattrs += 1\n",
    "                candi_text += predicate['object']\n",
    "                #candi_text += predicate['predicate']\n",
    "            numerical_f.append((i,candi_text,s['text']))\n",
    "            i+=1\n",
    "            #numerical_f.append(cal_similarity(candi_text,s['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.801 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.716 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.874 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.113 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.116 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.126 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "p = multiprocessing.Pool(6)\n",
    "res = p.map(cal_similarity, numerical_f)\n",
    "res = sorted(res,key=lambda x:x[0],reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nume_f = [x[1] for x in res]\n",
    "numerical_f = np.array(nume_f)\n",
    "for i in range(numerical_f.shape[1]):\n",
    "    data['numerical_%d'%i] = numerical_f[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate　type的one-hot特征\n",
    "data['type'] = data.apply(lambda x:kb[x['kb_id']]['type'][0],axis=1)\n",
    "#data = pd.concat([data,pd.get_dummies(data['type'])],axis=1)\n",
    "\n",
    "# target encoding\n",
    "data = data.merge(groupbytype,on='type',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['m_id'] = data['m_id']+max_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True,inplace=True)\n",
    "data.to_pickle('data/step1_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
