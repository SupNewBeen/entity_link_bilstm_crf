{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:u8\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "import gc,jieba\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = []\n",
    "# train_data_dict = {}\n",
    "# with open('data/raw_data/train.json','r') as f:\n",
    "#     for line in f:\n",
    "#         raw = json.loads(line)\n",
    "#         train_data.append(raw)\n",
    "#         train_data_dict[raw['text_id']] = raw\n",
    "# # 读取知识库\n",
    "# kb_data = []\n",
    "# kb = {}\n",
    "# with open('data/raw_data/kb_data','r') as f:\n",
    "#     for line in f:\n",
    "#         item = json.loads(line)\n",
    "#         kb[item['subject_id']] = item\n",
    "#         kb_data.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_data_dict = {}\n",
    "with open('data/raw_data/train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        raw = eval(str(json.loads(line)).lower())\n",
    "        train_data.append(raw)\n",
    "        #train_data_dict[raw['text_id']] = raw\n",
    "kb = {}\n",
    "with open('data/raw_data/kb_data', 'r') as f:\n",
    "    for line in f:\n",
    "        item = eval(str(json.loads(line)).lower())\n",
    "        kb[item['subject_id']] = item\n",
    "\n",
    "name_id = {}\n",
    "for kb_id in kb:\n",
    "    for item in kb[kb_id]['alias']:\n",
    "        if item not in name_id:\n",
    "            name_id[item] = [kb_id]\n",
    "        else:\n",
    "            name_id[item].append(kb_id)\n",
    "    if kb[kb_id]['subject'] not in name_id:\n",
    "        name_id[kb[kb_id]['subject']] = [kb_id]\n",
    "    else:\n",
    "        name_id[kb[kb_id]['subject']].append(kb_id)\n",
    "for id in name_id:\n",
    "    name_id[id] = sorted(list(set(name_id[id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO:\\n预测type,\\n引入tfidf\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "预测type,\n",
    "引入tfidf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_word(a, b):\n",
    "        common_count = 0\n",
    "        for word in a:\n",
    "            if word in b:\n",
    "                common_count += 1\n",
    "        return common_count\n",
    "def cal_similarity(params):\n",
    "    index,sentence_a,sentence_b = params\n",
    "    la = len(sentence_a)\n",
    "    lb = len(sentence_b)\n",
    "\n",
    "    char_a = [c for c in sentence_a]\n",
    "    char_b = [c for c in sentence_b]\n",
    "    word_a = jieba.lcut_for_search(sentence_a)\n",
    "    word_b = jieba.lcut_for_search(sentence_b)\n",
    "    bigram_a = [sentence_a[i:i + 2] for i in range(len(sentence_a) - 1)]\n",
    "    bigram_b = [sentence_b[i:i + 2] for i in range(len(sentence_b) - 1)]\n",
    "    trigram_a = [sentence_a[i:i + 3] for i in range(len(sentence_a) - 2)]\n",
    "    trigram_b = [sentence_b[i:i + 3] for i in range(len(sentence_b) - 2)]\n",
    "\n",
    "    common_word, common_word_score = 0, 0\n",
    "    for w in word_b:\n",
    "        if w in word_a:\n",
    "            common_word += 1\n",
    "            common_word_score += 1\n",
    "\n",
    "    common_c = get_common_word(char_a, char_b)\n",
    "    common_bigram = get_common_word(bigram_a, bigram_b)\n",
    "    common_trigram = get_common_word(trigram_a, trigram_b)\n",
    "\n",
    "    max_len = max(la, lb)\n",
    "    min_len = min(la, lb)\n",
    "    if max_len==0:\n",
    "        max_len=0.01\n",
    "    if min_len==0:\n",
    "        min_len = 0.01\n",
    "    numerical_feats = [max_len, min_len, common_word, common_word_score,\n",
    "                                common_word / min_len, common_word / max_len,\n",
    "                                common_word_score / min_len, common_word_score / max_len,\n",
    "                                common_c / min_len, common_c / max_len,\n",
    "                                common_bigram / min_len, common_bigram / max_len,\n",
    "                                common_trigram / min_len, common_trigram / max_len,\n",
    "                                common_word_score - common_word]\n",
    "    return index, numerical_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:17<00:00, 5236.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取candidate候选特征\n",
    "kb_column = []\n",
    "train_column = []\n",
    "text_id = []\n",
    "label_colum = []\n",
    "num_attrs = []\n",
    "num_abstract_words = []\n",
    "num_alias = []\n",
    "m_id = 0\n",
    "m_id_list = []\n",
    "num_candidate = []\n",
    "equal_subject = []\n",
    "shuminghao = [] \n",
    "entity_common = []\n",
    "len_mention = []\n",
    "mention_start = []\n",
    "numerical_f = []\n",
    "is_eng = []\n",
    "for s in tqdm(train_data):\n",
    "    mention_ner = s['mention_data']\n",
    "    m_list = []\n",
    "    for m in mention_ner:\n",
    "        m_list.append(m['mention'])\n",
    "    for m in mention_ner:\n",
    "        \n",
    "        if m['mention'] not in name_id : # 不能注销，因为有些实体没有or m['kb_id']=='nil':\n",
    "            continue\n",
    "        \n",
    "        if m['kb_id'] != 'nil':\n",
    "            name_list = [kb[m['kb_id']]['subject']]+kb[m['kb_id']]['alias']\n",
    "            if m['mention'] not in name_list:\n",
    "                    continue\n",
    "            # print('success')\n",
    "            \n",
    "        candidate_ids = name_id[m['mention']]\n",
    "        for m_candidate_id in candidate_ids:\n",
    "            # 统计配对特征\n",
    "            candidate_numattrs = 0\n",
    "            candidate_abstract_numwords = 0\n",
    "            candidate_detail = kb[m_candidate_id]\n",
    "            candi_text = ''\n",
    "            for predicate in candidate_detail['data']:\n",
    "                candidate_numattrs += 1\n",
    "                candi_text += predicate['object']\n",
    "                #candi_text += predicate['predicate']\n",
    "\n",
    "                if predicate['predicate'] == '摘要':\n",
    "                    candidate_abstract = predicate['object']\n",
    "                    candidate_abstract_numwords = len(candidate_abstract)\n",
    "#                 if predicate['predicate'] == '标签':\n",
    "#                     candidate_label += predicate['object']\n",
    "            if m_candidate_id==m['kb_id']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            train_column.append(m['mention'])\n",
    "            kb_column.append(m_candidate_id)\n",
    "            text_id.append(s['text_id'])\n",
    "            label_colum.append(label)\n",
    "            num_attrs.append(candidate_numattrs)\n",
    "            num_abstract_words.append(candidate_abstract_numwords)\n",
    "            num_alias.append(len(set(candidate_detail['alias']+[candidate_detail['subject']])))\n",
    "            m_id_list.append(m_id)\n",
    "            num_candidate.append(len(candidate_ids))\n",
    "            equal_subject.append(candidate_detail['subject']==m['mention'])\n",
    "            shuminghao.append('《'+candidate_detail['subject']+'》' in candidate_abstract)\n",
    "            \n",
    "#             if langid.classify(m['mention'])[0]=='en':\n",
    "#                 len_mention.append(len(m['mention'].split()))\n",
    "#                 is_eng.append(1)\n",
    "#             else:\n",
    "#                 len_mention.append(len(m['mention']))\n",
    "#                 is_eng.append(0)\n",
    "            len_mention.append(len(m['mention']))\n",
    "            mention_start.append(m['offset'])\n",
    "            \n",
    "            i = 0\n",
    "            for item in m_list:\n",
    "                if item in candi_text: i+=1\n",
    "            entity_common.append(i)\n",
    "            #numerical_f.append(cal_similarity(candi_text,s['text']))\n",
    "        m_id += 1\n",
    "        \n",
    "data = pd.DataFrame()\n",
    "data['text_id'] = text_id\n",
    "data['kb_id'] = kb_column\n",
    "data['train_mention'] = train_column\n",
    "data['label'] = label_colum\n",
    "data['num_attrs'] = num_attrs\n",
    "data['num_abstract_words'] = num_abstract_words\n",
    "data['num_alias'] = num_alias\n",
    "data['m_id'] = m_id_list\n",
    "data['num_candidates'] = num_candidate \n",
    "data['mention_equal_subject'] = equal_subject\n",
    "data['shuminghao'] = shuminghao\n",
    "data['entity_common']  = entity_common\n",
    "data['len_mention'] = len_mention\n",
    "data['mention_start'] = list(map(int,mention_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1583725\n",
       "Name: add_extra, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1583725, 15)\n",
      "(1566680, 15)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data[data.add_extra==0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8391.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取candidate候选特征\n",
    "numerical_f = []\n",
    "i = 0\n",
    "for s in tqdm(train_data):\n",
    "    mention_ner = s['mention_data']\n",
    "    m_list = []\n",
    "    for m in mention_ner:\n",
    "        m_list.append(m['mention'])\n",
    "    for m in mention_ner:\n",
    "        if m['mention'] not in name_id : # 不能注销，因为有些实体没有or m['kb_id']=='nil':\n",
    "            continue\n",
    "        \n",
    "        candidate_ids = name_id[m['mention']]\n",
    "        add_extra = 0 \n",
    "        if m['kb_id'] != 'nil':\n",
    "            name_list = [kb[m['kb_id']]['subject']]+kb[m['kb_id']]['alias']\n",
    "            \n",
    "            if m['mention'] not in name_list:\n",
    "                keep = False\n",
    "                for name_c in ''.join(name_list):\n",
    "                    if name_c in m['mention']:\n",
    "                        #print(name_c,m['mention'])\n",
    "                        keep = True\n",
    "                        break\n",
    "                if not keep:\n",
    "                    continue\n",
    "                candidate_ids = name_id[m['mention']]+[m['kb_id']]\n",
    "                add_extra = 1\n",
    "        for m_candidate_id in candidate_ids:\n",
    "            # 统计配对特征\n",
    "            candidate_numattrs = 0\n",
    "            candidate_abstract_numwords = 0\n",
    "            candidate_detail = kb[m_candidate_id]\n",
    "            candi_text = ''\n",
    "            for predicate in candidate_detail['data']:\n",
    "                candidate_numattrs += 1\n",
    "                candi_text += predicate['object']\n",
    "                #candi_text += predicate['predicate']\n",
    "            numerical_f.append((i,candi_text,s['text']))\n",
    "            i+=1\n",
    "            #numerical_f.append(cal_similarity(candi_text,s['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.090 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.216 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.372 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.189 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.427 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "p = multiprocessing.Pool(6)\n",
    "res = p.map(cal_similarity, numerical_f)\n",
    "res = sorted(res,key=lambda x:x[0],reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nume_f = [x[1] for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_f = np.array(nume_f)\n",
    "for i in range(numerical_f.shape[1]):\n",
    "    data['numerical_%d'%i] = numerical_f[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_columns = ['numerical_%d'%i for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[f_columns].to_pickle('data/numerical_f.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_nume_df = pd.read_pickle('data/numerical_f.pkl')\n",
    "# for item in f_columns:\n",
    "#     data[item] = f_nume_df[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/step1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/step1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhukaihua/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# 引入embedding特征\n",
    "mention_vector = np.load('entity_embedding/gensim_vector.npy')\n",
    "entity_embedding = Word2Vec.load('embedding/w2v.model')\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1566680, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def cal_distance(params) :\n",
    "    index,left,right = params\n",
    "#     left = entity_embedding.wv.word_vec(left)\n",
    "#     right = mention_vector(right)\n",
    "    return index,mse(left,right),cosine_similarity(left.reshape(1,-1),right.reshape(1,-1))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numlist = []\n",
    "for index,row in data.iterrows():\n",
    "    numlist.append((index,entity_embedding.wv.word_vec(row['kb_id']),mention_vector[index]))\n",
    "p = multiprocessing.Pool(6)\n",
    "res = p.map(cal_distance, numlist)\n",
    "res = sorted(res,key=lambda x:x[0],reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cos_distance'] = [x[1] for x in res]\n",
    "data['mse_distance'] = [x[2] for x in res]\n",
    "data['mse_distance'] = data['mse_distance'].apply(lambda x:x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/step2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate　type的one-hot特征\n",
    "data['type'] = data.apply(lambda x:kb[x['kb_id']]['type'][0],axis=1)\n",
    "#data = pd.concat([data,pd.get_dummies(data['type'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding\n",
    "data_groupbytype = data.groupby('type',as_index=False)['label'].agg({\n",
    "    'type_label_mean':'mean','type_label_count':'count'})\n",
    "data = data.merge(data_groupbytype,on='type',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True,inplace=True)\n",
    "data.to_pickle('features/1_stat_feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
