{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数值特征\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_data_dict = {}\n",
    "with open('data/raw_data/train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        raw = eval(str(json.loads(line)).lower())\n",
    "        train_data.append(raw)\n",
    "        #train_data_dict[raw['text_id']] = raw\n",
    "kb = {}\n",
    "with open('data/raw_data/kb_data', 'r') as f:\n",
    "    for line in f:\n",
    "        item = eval(str(json.loads(line)).lower())\n",
    "        kb[item['subject_id']] = item\n",
    "# 补充别名实体进入kb\n",
    "\n",
    "            \n",
    "name_id = {}\n",
    "for kb_id in kb:\n",
    "    for item in kb[kb_id]['alias']:\n",
    "        if item not in name_id:\n",
    "            name_id[item] = [kb_id]\n",
    "        else:\n",
    "            name_id[item].append(kb_id)\n",
    "    if kb[kb_id]['subject'] not in name_id:\n",
    "        name_id[kb[kb_id]['subject']] = [kb_id]\n",
    "    else:\n",
    "        name_id[kb[kb_id]['subject']].append(kb_id)\n",
    "for id in name_id:\n",
    "    name_id[id] = sorted(list(set(name_id[id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:13<00:00, 6460.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取candidate候选特征\n",
    "kb_column = []\n",
    "train_column = []\n",
    "text_id = []\n",
    "label_colum = []\n",
    "num_attrs = []\n",
    "num_abstract_words = []\n",
    "num_alias = []\n",
    "m_id = 0\n",
    "m_id_list = []\n",
    "num_candidate = []\n",
    "equal_subject = []\n",
    "shuminghao = [] \n",
    "entity_common = []\n",
    "len_mention = []\n",
    "mention_start = []\n",
    "numerical_f = []\n",
    "is_eng = []\n",
    "for s in tqdm(train_data):\n",
    "    mention_ner = s['mention_data']\n",
    "    m_list = []\n",
    "    for m in mention_ner:\n",
    "        m_list.append(m['mention'])\n",
    "    for m in mention_ner:\n",
    "        if m['mention'] not in name_id : # 不能注销，因为有些实体没有or m['kb_id']=='nil':\n",
    "            continue\n",
    "        if m['kb_id'] != 'nil':\n",
    "            name_list = [kb[m['kb_id']]['subject']]+kb[m['kb_id']]['alias']\n",
    "            if m['mention'] not in name_list:\n",
    "                continue \n",
    "        candidate_ids = name_id[m['mention']]\n",
    "        for m_candidate_id in candidate_ids:\n",
    "            # 统计配对特征\n",
    "            candidate_numattrs = 0\n",
    "            candidate_abstract_numwords = 0\n",
    "            candidate_detail = kb[m_candidate_id]\n",
    "            candi_text = ''\n",
    "            for predicate in candidate_detail['data']:\n",
    "                candidate_numattrs += 1\n",
    "                candi_text += predicate['object']\n",
    "                #candi_text += predicate['predicate']\n",
    "\n",
    "                if predicate['predicate'] == '摘要':\n",
    "                    candidate_abstract = predicate['object']\n",
    "                    candidate_abstract_numwords = len(candidate_abstract)\n",
    "#                 if predicate['predicate'] == '标签':\n",
    "#                     candidate_label += predicate['object']\n",
    "            if m_candidate_id==m['kb_id']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            train_column.append(m['mention'])\n",
    "            kb_column.append(m_candidate_id)\n",
    "            text_id.append(s['text_id'])\n",
    "            label_colum.append(label)\n",
    "            num_attrs.append(candidate_numattrs)\n",
    "            num_abstract_words.append(candidate_abstract_numwords)\n",
    "            num_alias.append(len(set(candidate_detail['alias']+[candidate_detail['subject']])))\n",
    "            m_id_list.append(m_id)\n",
    "\n",
    "        m_id += 1\n",
    "        \n",
    "data = pd.DataFrame()\n",
    "data['text_id'] = text_id\n",
    "data['kb_id'] = kb_column\n",
    "data['train_mention'] = train_column\n",
    "data['label'] = label_colum\n",
    "data['m_id'] = m_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ... 174073 174074 174075]\n",
      "(19791, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhukaihua/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/zhukaihua/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174076 174077 174078 ... 348149 348150 348151]\n",
      "(19651, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[348152 348153 348154 ... 522225 522226 522227]\n",
      "(18949, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[522228 522229 522230 ... 696301 696302 696303]\n",
      "(20126, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[696304 696305 696306 ... 870377 870378 870379]\n",
      "(19116, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 870380  870381  870382 ... 1044452 1044453 1044454]\n",
      "(19575, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1044455 1044456 1044457 ... 1218527 1218528 1218529]\n",
      "(19473, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1218530 1218531 1218532 ... 1392602 1392603 1392604]\n",
      "(19438, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1392605 1392606 1392607 ... 1566677 1566678 1566679]\n",
      "(19570, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# 流行度特征\n",
    "# 总流行度\n",
    "data_ctr = []\n",
    "kfold = KFold(n_splits=9,shuffle=False,random_state=2019)\n",
    "for train_index,test_index in kfold.split(data):\n",
    "    print(test_index)\n",
    "    stat_train_data,stat_test_data = data.loc[train_index,:],data.loc[test_index,:]\n",
    "    #print(test_data.head())\n",
    "    train_group = stat_train_data.groupby('kb_id',as_index=False)['label'].agg({'label_mean':'mean','label_max':'max',\n",
    "                                                                                'label_var':'var','label_count':'count'})\n",
    "    stat_test_data = stat_test_data.merge(train_group,on='kb_id',how='left')\n",
    "    print(stat_test_data[stat_test_data.label_mean.isnull()].shape)\n",
    "    train_group = stat_train_data.groupby(['kb_id','train_mention'])['label'].agg({'m_label_mean':'mean','m_label_max':'max',\n",
    "                                                                                   'm_label_var':'var','m_label_count':'count'})\n",
    "    stat_test_data = stat_test_data.merge(train_group,on=['kb_id','train_mention'],how='left')\n",
    "\n",
    "    # mean and var\n",
    "    to_rank = [ 'm_label_mean', 'm_label_max',\n",
    "           'm_label_var', 'm_label_count', \n",
    "              ]\n",
    "    for f in tqdm(to_rank):\n",
    "        data_group = stat_test_data.groupby('kb_id')[f].agg({'kb_'+f+'_var':'var','kb_'+f+'_mean':'mean','kb_'+f+'_max':'max',\n",
    "                                                       'kb_'+f+'_min':'min'})\n",
    "        stat_test_data = stat_test_data.merge(data_group,on='kb_id',how='left')\n",
    "\n",
    "    # rank 特征\n",
    "    to_rank = [ 'm_label_mean', 'm_label_max',\n",
    "           'm_label_var', 'm_label_count', \n",
    "              ]\n",
    "    for f in to_rank:\n",
    "        data_group = stat_test_data.groupby('kb_id')[f].rank(ascending=False)\n",
    "        stat_test_data['kbid_rank_'+f] = data_group\n",
    "    \n",
    "    data_ctr.append(stat_test_data)\n",
    "data_ctr = pd.concat(data_ctr,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank 特征\n",
    "to_rank = ['label_mean', 'label_max',\n",
    "       'label_var', 'label_count', 'm_label_mean', 'm_label_max',\n",
    "       'm_label_var', 'm_label_count','kb_m_label_mean_var',\n",
    "       'kb_m_label_mean_mean', 'kb_m_label_mean_max', 'kb_m_label_mean_min',\n",
    "       'kb_m_label_max_var', 'kb_m_label_max_mean', 'kb_m_label_max_max',\n",
    "       'kb_m_label_max_min', 'kb_m_label_var_var', 'kb_m_label_var_mean',\n",
    "       'kb_m_label_var_max', 'kb_m_label_var_min', 'kb_m_label_count_var',\n",
    "       'kb_m_label_count_mean', 'kb_m_label_count_max', 'kb_m_label_count_min',\n",
    "       'kbid_rank_m_label_mean', 'kbid_rank_m_label_max',\n",
    "       'kbid_rank_m_label_var', 'kbid_rank_m_label_count'\n",
    "          ]\n",
    "for f in to_rank:\n",
    "    data_group = data_ctr.groupby('m_id')[f].rank(ascending=False)\n",
    "    data_ctr['mid_rank_'+f] = data_group\n",
    "    try:\n",
    "        data_ctr['mid_rank_'+f] = data_ctr['mid_rank_'+f].astype('int32')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/zhukaihua/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \n",
      "100%|██████████| 8/8 [00:11<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean and var\n",
    "to_rank = [ 'label_mean', 'label_max',\n",
    "       'label_var', 'label_count', 'm_label_mean', 'm_label_max',\n",
    "       'm_label_var', 'm_label_count'\n",
    "          ]\n",
    "for f in tqdm(to_rank):\n",
    "    data_group = data_ctr.groupby('m_id')[f].agg({'mid_'+f+'_var':'var','mid_'+f+'_mean':'mean',\n",
    "                                                  'mid_'+f+'_max':'max','mid_'+f+'_min':'min'})\n",
    "    data_ctr = data_ctr.merge(data_group,on='m_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctr 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data_ctr.columns:\n",
    "    if data_ctr.dtypes[item]=='float64':\n",
    "        data_ctr[item] = data_ctr[item].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ctr.reset_index(drop=True,inplace=True)\n",
    "data_ctr.to_pickle('data/step2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhukaihua/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1494: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_pickle('data/step1_test.pkl').loc[:,['text_id','kb_id','train_mention','m_id','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764217, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42625, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhukaihua/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/zhukaihua/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# 流行度特征\n",
    "# 总流行度\n",
    "\n",
    "\n",
    "train_group = data.groupby('kb_id',as_index=False)['label'].agg({'label_mean':'mean','label_max':'max',\n",
    "                                                                            'label_var':'var','label_count':'count'})\n",
    "test_data = test_data.merge(train_group,on='kb_id',how='left')\n",
    "print(test_data[test_data.label_mean.isnull()].shape)\n",
    "\n",
    "train_group = data.groupby(['kb_id','train_mention'])['label'].agg({'m_label_mean':'mean','m_label_max':'max',\n",
    "                                                                               'm_label_var':'var','m_label_count':'count'})\n",
    "test_data = test_data.merge(train_group,on=['kb_id','train_mention'],how='left')\n",
    "\n",
    "# mean and var\n",
    "to_rank = [ 'm_label_mean', 'm_label_max',\n",
    "       'm_label_var', 'm_label_count', \n",
    "          ]\n",
    "for f in tqdm(to_rank):\n",
    "    data_group = test_data.groupby('kb_id')[f].agg({'kb_'+f+'_var':'var','kb_'+f+'_mean':'mean','kb_'+f+'_max':'max',\n",
    "                                                   'kb_'+f+'_min':'min'})\n",
    "    test_data = test_data.merge(data_group,on='kb_id',how='left')\n",
    "\n",
    "# rank 特征\n",
    "to_rank = [ 'm_label_mean', 'm_label_max',\n",
    "       'm_label_var', 'm_label_count', \n",
    "          ]\n",
    "for f in to_rank:\n",
    "    data_group = test_data.groupby('kb_id')[f].rank(ascending=False)\n",
    "    test_data['kbid_rank_'+f] = data_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764217, 33)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/zhukaihua/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "100%|██████████| 8/8 [00:05<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# rank 特征\n",
    "to_rank = ['label_mean', 'label_max',\n",
    "       'label_var', 'label_count', 'm_label_mean', 'm_label_max',\n",
    "       'm_label_var', 'm_label_count','kb_m_label_mean_var',\n",
    "       'kb_m_label_mean_mean', 'kb_m_label_mean_max', 'kb_m_label_mean_min',\n",
    "       'kb_m_label_max_var', 'kb_m_label_max_mean', 'kb_m_label_max_max',\n",
    "       'kb_m_label_max_min', 'kb_m_label_var_var', 'kb_m_label_var_mean',\n",
    "       'kb_m_label_var_max', 'kb_m_label_var_min', 'kb_m_label_count_var',\n",
    "       'kb_m_label_count_mean', 'kb_m_label_count_max', 'kb_m_label_count_min',\n",
    "       'kbid_rank_m_label_mean', 'kbid_rank_m_label_max',\n",
    "       'kbid_rank_m_label_var', 'kbid_rank_m_label_count'\n",
    "          ]\n",
    "for f in to_rank:\n",
    "    data_group = test_data.groupby('m_id')[f].rank(ascending=False)\n",
    "    test_data['mid_rank_'+f] = data_group\n",
    "# mean and var\n",
    "to_rank = [ 'label_mean', 'label_max',\n",
    "       'label_var', 'label_count', 'm_label_mean', 'm_label_max',\n",
    "       'm_label_var', 'm_label_count'\n",
    "          ]\n",
    "for f in tqdm(to_rank):\n",
    "    data_group = test_data.groupby('m_id')[f].agg({'mid_'+f+'_var':'var','mid_'+f+'_mean':'mean',\n",
    "                                                  'mid_'+f+'_max':'max','mid_'+f+'_min':'min'})\n",
    "    test_data = test_data.merge(data_group,on='m_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.reset_index(drop=True,inplace=True)\n",
    "test_data.to_pickle('data/step2_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
